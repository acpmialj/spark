{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e559ac65-ea12-4664-9cc0-069a30b602a1",
   "metadata": {},
   "source": [
    "# Contar palabras con Structured Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d25865b2-03a5-4daf-ac76-6b5bbea0c814",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing words.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile words.txt\n",
    "value\n",
    "A B C A\n",
    "A B A A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec1cb00f-cb41-485d-b12f-059e3abf8291",
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Se copia el archivo al HDFS\n",
    "##\n",
    "!hdfs dfs -rm -f /tmp/words.txt\n",
    "!hdfs dfs -copyFromLocal words.txt /tmp/words.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "74a013a9-f283-4d9a-a640-11aeb274efcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "##\n",
    "## Se inicia la aplicación en PySpark\n",
    "##\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "sparkConf = SparkConf().setAppName(\"My SparkQL Application\")\n",
    "sc = SparkContext(conf=sparkConf)\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d46f9790-0aaa-4159-8ba4-01031aaca261",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|  value|\n",
      "+-------+\n",
      "|A B C A|\n",
      "|A B A A|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##\n",
    "## Se lee el archivo del hdfs en formato CSV.\n",
    "## Cada fila del DataFrame es un renglón del archivo\n",
    "##\n",
    "df = spark.read.load(\n",
    "    \"/tmp/words.txt\",\n",
    "    format=\"csv\",\n",
    "    sep=\",\",\n",
    "    inferSchema=\"true\",\n",
    "    header=\"true\")\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c21cc726-9f51-4040-945e-b7de9068a1b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "|word|\n",
      "+----+\n",
      "|   A|\n",
      "|   B|\n",
      "|   C|\n",
      "|   A|\n",
      "|   A|\n",
      "|   B|\n",
      "|   A|\n",
      "|   A|\n",
      "+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import explode\n",
    "from pyspark.sql.functions import split\n",
    "\n",
    "##\n",
    "## La función split parte cada línea de texto por los espacios en\n",
    "## blanco, retornando un vector; por ejemplo, para la primera\n",
    "## línea retorna ['A', 'B', 'C', 'A']. Seguidamente, la función\n",
    "## explode genera un registro por cada elemento del vector, tal\n",
    "## como se muestra a continuación.\n",
    "##\n",
    "words = df.select(\n",
    "   explode(\n",
    "       split(df.value, \" \")\n",
    "   ).alias(\"word\")\n",
    ")\n",
    "\n",
    "words.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "810e145a-82c6-4999-a2bb-567e62866428",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "|word|count|\n",
      "+----+-----+\n",
      "|   B|    2|\n",
      "|   C|    1|\n",
      "|   A|    5|\n",
      "+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##\n",
    "## Para realizar el conteo propiamente, se realizar un\n",
    "## groupBy por letra, y se cuenta la cantidad de registros\n",
    "## por grupo usando la función `count`.\n",
    "##\n",
    "wordCounts = words.groupBy(\"word\").count()\n",
    "wordCounts.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c02a6589-39d0-4324-a826-6ea51a163e6a",
   "metadata": {},
   "source": [
    "## Ahora vamos a usar Spark Streaming\n",
    "Nótese que escribimos un fichero Python a ejecutar fuera de Jupyter. Necesitaremos ejecutar algunas cosas desde un terminal. Jupyter nos lo permite, desde una pestaña nueva. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "36bb22d8-d05e-4a75-959b-9e3731f8acf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing wc-pyspark.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile wc-pyspark.py\n",
    "\n",
    "## Identico ------>>>\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import explode\n",
    "from pyspark.sql.functions import split\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"StructuredNetworkWordCount\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "## <<<------\n",
    "\n",
    "\n",
    "##\n",
    "## Los datos se leen desde un flujo de entrada en vez de un archivo\n",
    "## en disco. Para ello, se crea un Dstream de entrada que representa las líneas\n",
    "## de texto de entrada, las cuales son leídas desde una conexión a\n",
    "## localhost:9999. El Dstream puede considerarse como un DataFrame\n",
    "## infinito, donde los nuevos datos se van adicionando al final.\n",
    "##\n",
    "df = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"socket\") \\\n",
    "    .option(\"host\", \"localhost\") \\\n",
    "    .option(\"port\", 9999) \\\n",
    "    .load()\n",
    "\n",
    "\n",
    "## Identico ------>>>\n",
    "\n",
    "words = df.select(\n",
    "   explode(\n",
    "       split(df.value, \" \")\n",
    "   ).alias(\"word\")\n",
    ")\n",
    "\n",
    "wordCounts = words.groupBy(\"word\").count()\n",
    "\n",
    "## <<<------\n",
    "\n",
    "\n",
    "##\n",
    "## Crea un Dstream de salida a la consola, en la que se van\n",
    "## escribiendo los resultados a medida que se van ingresando\n",
    "## datos.\n",
    "##\n",
    "query = wordCounts \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .format(\"console\") \\\n",
    "    .start()\n",
    "\n",
    "query.awaitTermination()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d30b0c0-08fb-4845-bba4-ad02953070d8",
   "metadata": {},
   "source": [
    "Ahora abramos DOS pestañas terminal. \n",
    "\n",
    "En una de ellas ejecutamos \"nc -lk 9999\". Esto lanza un servidor de red en el puerto 9999. El servidor queda a la espera de conexiones (-l, listening). Cuando una conexión se cierra, queda a la espera de nuevas (-k).\n",
    "\n",
    "En la otra pestaña terminal lanzamos nuestra aplicación Spark Streaming. La misma se va a conectar a nuestro servidor, desde donde recibirá datos a procesar. \n",
    "\n",
    "Una vez lanzadas las dos partes, todo lo que escribamos en la primera pestaña (donde ejecutamos \"nc\") será procesado por la aplicación. Lo que escribamos será una secuencia de palabras. La aplicación Spark contará las instancias. En cada batch, se actualizará el conteo. Inicialmente aparecerá una lista vacía. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce2dddb-9606-42e4-b896-a7a9bad0b929",
   "metadata": {},
   "source": [
    "Esto es lo que tecleamos en una pestaña:\n",
    "    \n",
    "```sh\n",
    "root@24730d8dd172:/workspace# nc -lk 9999\n",
    "A B C A\n",
    "A B A A\n",
    "El perro de san roque no tiene rabo porque es\n",
    "perro y el rabo se lo han cortado\n",
    "pobre san roque y su perro sin rabo\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6136853c-7c25-4a92-84dc-c2aa91d7dfb4",
   "metadata": {},
   "source": [
    "Y esto es (un extracto) de lo que vemos en la otra:\n",
    "```sh\n",
    "...\n",
    "|     se|    1|\n",
    "|     El|    1|\n",
    "|     lo|    1|\n",
    "+-------+-----+\n",
    "\n",
    "-------------------------------------------\n",
    "Batch: 5\n",
    "-------------------------------------------\n",
    "+-------+-----+\n",
    "|   word|count|\n",
    "+-------+-----+\n",
    "|    san|    2|\n",
    "|  tiene|    1|\n",
    "| porque|    1|\n",
    "|      B|    2|\n",
    "|  roque|    2|\n",
    "|  perro|    3|\n",
    "|     de|    1|\n",
    "|   rabo|    3|\n",
    "|     es|    1|\n",
    "|      C|    1|\n",
    "|     el|    1|\n",
    "|     su|    1|\n",
    "|      A|    5|\n",
    "|      y|    2|\n",
    "|cortado|    1|\n",
    "|    han|    1|\n",
    "|  pobre|    1|\n",
    "|     no|    1|\n",
    "|    sin|    1|\n",
    "|     se|    1|\n",
    "+-------+-----+\n",
    "only showing top 20 rows\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15663223-f75c-487a-9fda-f91505788991",
   "metadata": {},
   "source": [
    "La aplicación hace lo siguiente:\n",
    "    \n",
    "1. Crea una SparkSession\n",
    "2. Crea un Dstream \"df\", leyendo de un stream. El tipo de Dstream es \"socket\", es decir, se lee desde un servidor de red. La conexión queda abierta permanentemente hasta que se cierre el Dstream. \"df\" está en actualización permanente.\n",
    "3. El Dstream \"df\" se procesa exactamente igual que en el caso batch: cada línea se separa en palabras usando el espacio como separador. Por cada línea se devuelve una lista de palabras (explode). La lista de palabras se guarda en \"words\". Como \"words\" está asociado a \"df\", su actualización también es permanente.\n",
    "4. Igual que en el caso batch, se agrupan las palabras iguales, generando \"wordCounts\" -- que también se actualiza de forma permanente.\n",
    "5. Se crea una salida \"query\" de tipo Dstream, permanente, por la consola. \n",
    "    \n",
    "Todo queda en marcha, hasta que termina la operación \"query\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21ce7a10-d317-42ca-8a58-f53e15f55ccd",
   "metadata": {},
   "source": [
    "Spark Streaming utiliza varias fuentes de entrada (source):\n",
    "\n",
    "1. Socket. Se lee de un puerto de red.\n",
    "2. File. Se monitoriza un directorio. Por cada fichero que se añade, se actualiza el stream.\n",
    "3. Kafka. Se lee desde Kafka.\n",
    "4. Rate. Se generan datos a una velocidad de N filas/segundo. Cada campo tiene un timestamp y un valor. \n",
    "\n",
    "En cuanto a las salidas, se puede especificar el modo:\n",
    "    \n",
    "1. Complete. Cada vez que hay una actualización, se escribe el DataFrame completo\n",
    "2. Append. Se escriben las filas nuevas. Esto es útil si las anteriores ya no cambian. \n",
    "3. Update. Se escriben las filas que han cambiado. \n",
    "\n",
    "Y también se puede especificar hacia dónde se envía (sink):\n",
    "\n",
    "1. File. Hay que indicar el formato: csv, orc, json, parquet\n",
    "2. Kafka\n",
    "3. Console -- solo para depuración\n",
    "4. Custom\n",
    "\n",
    "En cuanto a cuándo se ejecuta un procesamiento, se puede indicar:\n",
    "\n",
    "1. Unspecified. Por omisión. Micro-lotes, que se ejecutan cuando el anterior ha terminado. Un micro-lote es un conjunto de datos que llegan a la vez. \n",
    "2. Fixed interval micro-batches. Se especifica el tiempo que dura cada micro-batch (por ejemplo, 2 segundos)\n",
    "3. One-time micro-batch. Se ejecuta una vez -- habrá algún proceso externo que lo lance\n",
    "4. Continuous. Nuevo, experimental. Se usa para emular un proceso continuo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "979514bc-beda-4278-a5aa-b212f2e388ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
