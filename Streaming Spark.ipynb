{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e559ac65-ea12-4664-9cc0-069a30b602a1",
   "metadata": {},
   "source": [
    "# Contar palabras con Structured Streaming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a3490c8",
   "metadata": {},
   "source": [
    "## Conteo batch\n",
    "Primero haremos un conteo de palabras en modo batch, para ver las diferencias con el modo streaming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d25865b2-03a5-4daf-ac76-6b5bbea0c814",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing words.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile words.txt\n",
    "value\n",
    "A B C A\n",
    "A B A A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec1cb00f-cb41-485d-b12f-059e3abf8291",
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Se copia el archivo al HDFS\n",
    "##\n",
    "!hdfs dfs -rm -f /tmp/words.txt\n",
    "!hdfs dfs -copyFromLocal words.txt /tmp/words.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "74a013a9-f283-4d9a-a640-11aeb274efcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "##\n",
    "## Se inicia la aplicación en PySpark\n",
    "##\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "sparkConf = SparkConf().setAppName(\"My SparkQL Application\")\n",
    "sc = SparkContext(conf=sparkConf)\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d46f9790-0aaa-4159-8ba4-01031aaca261",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|  value|\n",
      "+-------+\n",
      "|A B C A|\n",
      "|A B A A|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##\n",
    "## Se lee el archivo del hdfs en formato CSV.\n",
    "## Cada fila del DataFrame es un renglón del archivo\n",
    "##\n",
    "df = spark.read.load(\n",
    "    \"/tmp/words.txt\",\n",
    "    format=\"csv\",\n",
    "    sep=\",\",\n",
    "    inferSchema=\"true\",\n",
    "    header=\"true\")\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c21cc726-9f51-4040-945e-b7de9068a1b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "|word|\n",
      "+----+\n",
      "|   A|\n",
      "|   B|\n",
      "|   C|\n",
      "|   A|\n",
      "|   A|\n",
      "|   B|\n",
      "|   A|\n",
      "|   A|\n",
      "+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import explode\n",
    "from pyspark.sql.functions import split\n",
    "\n",
    "##\n",
    "## La función split parte cada línea de texto por los espacios en\n",
    "## blanco, retornando un vector; por ejemplo, para la primera\n",
    "## línea retorna ['A', 'B', 'C', 'A']. Seguidamente, la función\n",
    "## explode genera un registro por cada elemento del vector, tal\n",
    "## como se muestra a continuación.\n",
    "##\n",
    "words = df.select(\n",
    "   explode(\n",
    "       split(df.value, \" \")\n",
    "   ).alias(\"word\")\n",
    ")\n",
    "\n",
    "words.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "810e145a-82c6-4999-a2bb-567e62866428",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "|word|count|\n",
      "+----+-----+\n",
      "|   B|    2|\n",
      "|   C|    1|\n",
      "|   A|    5|\n",
      "+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##\n",
    "## Para realizar el conteo propiamente, se realizar un\n",
    "## groupBy por letra, y se cuenta la cantidad de registros\n",
    "## por grupo usando la función `count`.\n",
    "##\n",
    "wordCounts = words.groupBy(\"word\").count()\n",
    "wordCounts.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c02a6589-39d0-4324-a826-6ea51a163e6a",
   "metadata": {},
   "source": [
    "## Conteo con Spark Streaming\n",
    "Usando las ideas del apartado anterior, haremos un conteo de palabras en streaming. Las palabras nos irán llegando desde un servidor de red \"localhost:9999\". El ejemplo no se ejecuta desde este cuaderno: se escribe un fichero Python que se ejecuta desde un terminal (shell) fuera de Jupyter. Jupyter nos permite lanzar los terminales que necesitemos. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "36bb22d8-d05e-4a75-959b-9e3731f8acf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing wc-pyspark.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile wc-pyspark.py\n",
    "\n",
    "## Identico ------>>>\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import explode\n",
    "from pyspark.sql.functions import split\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"StructuredNetworkWordCount\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "## <<<------\n",
    "\n",
    "\n",
    "##\n",
    "## Los datos se leen desde un flujo de entrada en vez de un archivo\n",
    "## en disco. Para ello, se crea un Dstream de entrada que representa las líneas\n",
    "## de texto de entrada, las cuales son leídas desde una conexión a\n",
    "## localhost:9999. El Dstream puede considerarse como un DataFrame\n",
    "## infinito, donde los nuevos datos se van adicionando al final.\n",
    "##\n",
    "df = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"socket\") \\\n",
    "    .option(\"host\", \"localhost\") \\\n",
    "    .option(\"port\", 9999) \\\n",
    "    .load()\n",
    "\n",
    "\n",
    "## Identico ------>>>\n",
    "\n",
    "words = df.select(\n",
    "   explode(\n",
    "       split(df.value, \" \")\n",
    "   ).alias(\"word\")\n",
    ")\n",
    "\n",
    "wordCounts = words.groupBy(\"word\").count()\n",
    "\n",
    "## <<<------\n",
    "\n",
    "\n",
    "##\n",
    "## Crea un Dstream de salida a la consola, en la que se van\n",
    "## escribiendo los resultados a medida que se van ingresando\n",
    "## datos.\n",
    "##\n",
    "query = wordCounts \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .format(\"console\") \\\n",
    "    .start()\n",
    "\n",
    "query.awaitTermination()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d30b0c0-08fb-4845-bba4-ad02953070d8",
   "metadata": {},
   "source": [
    "Ahora abramos DOS pestañas terminal. \n",
    "\n",
    "En una de ellas ejecutamos \"nc -lk 9999\". Esto lanza un servidor de red en el puerto 9999. El servidor queda a la espera de conexiones (-l, listening). Cuando una conexión se cierra, queda a la espera de nuevas (-k).\n",
    "\n",
    "En la otra pestaña terminal lanzamos nuestra aplicación Spark Streaming \"python3 wc-pyspark.py\". La misma se va a conectar a nuestro servidor, desde donde recibirá datos a procesar. \n",
    "\n",
    "Una vez lanzadas las dos partes, todo lo que escribamos en la primera pestaña (donde ejecutamos \"nc\") será procesado por la aplicación. Lo que escribamos será una secuencia de palabras. La aplicación Spark contará las instancias. En cada batch, se actualizará el conteo. Inicialmente aparecerá una lista vacía. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce2dddb-9606-42e4-b896-a7a9bad0b929",
   "metadata": {},
   "source": [
    "Esto es lo que tecleamos en una pestaña:\n",
    "    \n",
    "```sh\n",
    "root@24730d8dd172:/workspace# nc -lk 9999\n",
    "A B C A\n",
    "A B A A\n",
    "El perro de san roque no tiene rabo porque es\n",
    "perro y el rabo se lo han cortado\n",
    "pobre san roque y su perro sin rabo\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6136853c-7c25-4a92-84dc-c2aa91d7dfb4",
   "metadata": {},
   "source": [
    "Y esto es (un extracto) de lo que vemos en la otra:\n",
    "```sh\n",
    "...\n",
    "|     se|    1|\n",
    "|     El|    1|\n",
    "|     lo|    1|\n",
    "+-------+-----+\n",
    "\n",
    "-------------------------------------------\n",
    "Batch: 5\n",
    "-------------------------------------------\n",
    "+-------+-----+\n",
    "|   word|count|\n",
    "+-------+-----+\n",
    "|    san|    2|\n",
    "|  tiene|    1|\n",
    "| porque|    1|\n",
    "|      B|    2|\n",
    "|  roque|    2|\n",
    "|  perro|    3|\n",
    "|     de|    1|\n",
    "|   rabo|    3|\n",
    "|     es|    1|\n",
    "|      C|    1|\n",
    "|     el|    1|\n",
    "|     su|    1|\n",
    "|      A|    5|\n",
    "|      y|    2|\n",
    "|cortado|    1|\n",
    "|    han|    1|\n",
    "|  pobre|    1|\n",
    "|     no|    1|\n",
    "|    sin|    1|\n",
    "|     se|    1|\n",
    "+-------+-----+\n",
    "only showing top 20 rows\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15663223-f75c-487a-9fda-f91505788991",
   "metadata": {},
   "source": [
    "La aplicación hace lo siguiente:\n",
    "    \n",
    "1. Crea una SparkSession\n",
    "2. Crea un Dstream de entrada \"df\". El tipo de Dstream es \"socket\", es decir, se reciben datos desde un servidor de red. La conexión queda abierta permanentemente hasta que se cierre el Dstream. \"df\" está en actualización permanente.\n",
    "3. El Dstream \"df\" se procesa exactamente igual que en el caso batch: cada línea se separa en palabras usando el espacio como separador. Por cada línea se devuelve una lista de palabras (explode). La lista de palabras se guarda en \"words\". Como \"words\" está asociado a \"df\", su actualización también es permanente.\n",
    "4. Igual que en el caso batch, se agrupan las palabras iguales, generando \"wordCounts\" -- que también se actualiza de forma permanente.\n",
    "5. Se crea un Dstream de salida \"query\", permanente, por la consola. \n",
    "    \n",
    "Todo queda en marcha, hasta que termina la operación \"query\".\n",
    "\n",
    "La ejecución será en micro-batches. Cada línea que se teclee en el terminal donde está el servidor de red activará un \"trigger\" de terminación del micro-batch. Además, la salida es en modo \"complete\", lo que quiere decir que, tras cada trigger, se volcará por pantalla la tabla de conteo *completa*. Se podría especificar \"append\" (solo se volcarían las filas nuevas) o \"update\" (se volcarían las filas que cambian, sean nuevas o viejas). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21ce7a10-d317-42ca-8a58-f53e15f55ccd",
   "metadata": {},
   "source": [
    "\n",
    "En cuanto a cuándo se ejecuta un procesamiento, se puede indicar en la consulta lo siguiente (poniendo \".trigger(...))\n",
    "\n",
    "1. Unspecified. Por omisión. Micro-lotes, que se ejecutan cuando el anterior ha terminado. Un micro-lote es un conjunto de datos que llegan a la vez. Es lo usado en este ejemplo.\n",
    "2. Fixed interval micro-batches. Se especifica el tiempo que dura cada micro-batch (por ejemplo, 2 segundos): .trigger(processingTime='2 seconds')\n",
    "3. AvailableNow: .trigger(availableNow=True)\n",
    "4. Continuous. Nuevo, experimental. Se usa para emular un proceso continuo. El parámetro es el intervalo entre checkpoints: .trigger(continuous='1 second')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "979514bc-beda-4278-a5aa-b212f2e388ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
